{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install librosa scipy numpy matplotlib\n"
      ],
      "metadata": {
        "id": "khzQkJTl4Ahh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.fftpack import fft\n",
        "\n",
        "# Function to load audio and compute pitch (fundamental frequency)\n",
        "def analyze_audio(file_path):\n",
        "    # Load the audio file\n",
        "    y, sr = librosa.load(file_path, sr=None)\n",
        "\n",
        "    # Estimate the pitch (fundamental frequency)\n",
        "    pitches, magnitudes = librosa.core.piptrack(y=y, sr=sr ,  ref=np.max)\n",
        "    print(pitches.shape)\n",
        "    print(magnitudes.shape )\n",
        "    # Extract the most prominent pitch over the entire file\n",
        "    pitch = []\n",
        "    for t in range(pitches.shape[1]):\n",
        "        index = magnitudes[:, t].argmax()\n",
        "        pitch_freq = pitches[index, t]\n",
        "        if pitch_freq > 0:\n",
        "            pitch.append(pitch_freq)\n",
        "\n",
        "    # Compute the average pitch (fundamental frequency)\n",
        "    avg_pitch = np.max(pitch) if len(pitch) > 0 else 0\n",
        "    var_pitch = np.var(pitch) if len(pitch) > 0 else 0\n",
        "\n",
        "    # Compute FFT\n",
        "    N = len(y)\n",
        "    yf = fft(y)\n",
        "    xf = np.linspace(0.0, sr / 2.0, N // 2)\n",
        "\n",
        "    return avg_pitch, var_pitch , xf, 2.0/N * np.abs(yf[:N // 2])\n",
        "\n",
        "# Function to plot FFT\n",
        "def plot_fft(xf, yf, title):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(xf, yf)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Frequency (Hz)')\n",
        "    plt.ylabel('Magnitude')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Analyze each file\n",
        "audio_files = {\n",
        "    \"Speaker 1\": \"mom.mp3\",   # Path to speaker 1 file\n",
        "    \"Speaker 2\": \"son.mp3\",   # Path to speaker 2 file\n",
        "    \"Both Speakers\": \"0929.wav\"  # Path to file where both are speaking\n",
        "}\n",
        "\n",
        "# Process each audio file\n",
        "for label, file_path in audio_files.items():\n",
        "    avg_pitch, var_pitch , xf, yf = analyze_audio(file_path)\n",
        "    print(f\"{label} - Average Pitch: {avg_pitch:.2f} Hz\")\n",
        "    print(f\"{label} - Variance Pitch: {var_pitch:.2f} Hz\")\n",
        "    plot_fft(xf, yf, f\"FFT of {label}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "BfF8JrLr4C5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gXMBwc9qtB1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from librosa.feature import mfcc\n",
        "from scipy.fft import fft, fftfreq\n",
        "\n",
        "# Function to calculate pitch using librosa's piptrack\n",
        "def calculate_pitch(audio, sr):\n",
        "    pitches, magnitudes = librosa.core.piptrack(y=audio, sr=sr)\n",
        "    pitch_values = []\n",
        "    for t in range(pitches.shape[1]):\n",
        "        index = magnitudes[:, t].argmax()\n",
        "        pitch = pitches[index, t]\n",
        "        if pitch > 0:\n",
        "            pitch_values.append(pitch)\n",
        "    return np.mean(pitch_values) if len(pitch_values) > 0 else None\n",
        "\n",
        "# Function to plot FFT for the audio signal\n",
        "def plot_fft(audio, sr, title):\n",
        "    # Perform FFT\n",
        "    N = len(audio)\n",
        "    yf = fft(audio)\n",
        "    xf = fftfreq(N, 1 / sr)\n",
        "\n",
        "    # Plot the results\n",
        "    plt.plot(xf[:N // 2], np.abs(yf[:N // 2]))\n",
        "    plt.title(f'Frequency Spectrum (FFT) - {title}')\n",
        "    plt.xlabel('Frequency (Hz)')\n",
        "    plt.ylabel('Amplitude')\n",
        "    plt.show()\n",
        "\n",
        "# Function to plot MFCCs\n",
        "def plot_mfcc(audio, sr, title):\n",
        "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
        "    print(mfccs.shape)\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    librosa.display.specshow(mfccs, x_axis='time')\n",
        "    plt.colorbar()\n",
        "    plt.title(f'MFCC - {title}')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Main processing function\n",
        "def process_audio(file_paths):\n",
        "    for i, file_path in enumerate(file_paths):\n",
        "        title = f\"Speaker {i+1}\" if i < 2 else \"Both Speakers\"\n",
        "        print(f\"\\nProcessing {title}:\")\n",
        "\n",
        "        # Load the audio file\n",
        "        audio, sr = librosa.load(file_path, sr=None)\n",
        "\n",
        "        # Calculate pitch\n",
        "        pitch = calculate_pitch(audio, sr)\n",
        "        print(f\"Pitch (mean): {pitch:.2f} Hz\")\n",
        "\n",
        "        # Plot FFT\n",
        "        plot_fft(audio, sr, title)\n",
        "\n",
        "        # Plot MFCC\n",
        "        plot_mfcc(audio, sr, title)\n",
        "\n",
        "# Provide the paths to your audio files\n",
        "file_paths = [\n",
        "    \"mom.mp3\",  # Speaker 1\n",
        "    \"son.mp3\",  # Speaker 2\n",
        "    \"0929.wav\"  # Both speakers together\n",
        "]\n",
        "\n",
        "# Process all files\n",
        "process_audio(file_paths)\n"
      ],
      "metadata": {
        "id": "d0bA08kPtCdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "37bRxh00ygFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load and preprocess the audio files\n",
        "def load_audio(file_path):\n",
        "    y, sr = librosa.load(file_path, sr=None)\n",
        "    print(len(y))\n",
        "    print(sr)\n",
        "    plt.plot(y)\n",
        "    return y, sr\n",
        "\n",
        "# Extract pitch using librosa's piptrack method\n",
        "def extract_pitch_segment(y , sr , title):\n",
        "    pitches, magnitudes = librosa.core.piptrack(y=y, sr=sr)\n",
        "    pitch = []\n",
        "    for t in range(pitches.shape[1]):\n",
        "        index = magnitudes[:, t].argmax()\n",
        "        pitch_value = pitches[index, t]\n",
        "        if pitch_value > 0:\n",
        "            pitch.append(pitch_value)\n",
        "\n",
        "    print(f\"Speaker : {title}\")\n",
        "    print(f\"Pitch : {np.mean(pitch)}\")\n",
        "    print(f\"std : {np.std(pitch)}\")\n",
        "    print(f\"Max : {np.max(pitch)}\")\n",
        "    print(f\"len : {len(pitch)}\")\n",
        "    return np.mean(pitch)\n",
        "\n",
        "# Perform FFT and plot the frequency spectrum\n",
        "def plot_fft(y, sr, title):\n",
        "    n = len(y)\n",
        "    fft = np.fft.fft(y)\n",
        "    fft_freq = np.fft.fftfreq(n, d=1/sr)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(fft_freq[:n // 2], np.abs(fft)[:n // 2])  # Plot positive frequencies\n",
        "    plt.title(f\"FFT of {title}\")\n",
        "    plt.xlabel(\"Frequency (Hz)\")\n",
        "    plt.ylabel(\"Magnitude\")\n",
        "    plt.show()\n",
        "\n",
        "# Split the audio into segments and calculate MFCCs\n",
        "def extract_mfcc(segments, sr):\n",
        "    mfccs = []\n",
        "    for segment in segments:\n",
        "        mfcc = librosa.feature.mfcc(y= segment, sr=sr, n_mfcc=13)\n",
        "        mfcc_mean = np.mean(mfcc, axis=1)\n",
        "        mfccs.append(mfcc_mean)\n",
        "    return np.array(mfccs)\n",
        "\n",
        "def extract_pitch(segments , sr , title):\n",
        "    pitch = []\n",
        "    pitch_segment = 0\n",
        "    for segment in segments:\n",
        "        pitch_segment = extract_pitch_segment(segment , sr , title)\n",
        "        pitch.append(pitch_segment)\n",
        "    return pitch\n",
        "\n",
        "def split_to_Segments(y, sr, segment_length=0.5):\n",
        "    segments = []\n",
        "    hop_length = int(sr * segment_length)\n",
        "    for i in range(0, len(y), hop_length):\n",
        "        segment = y[i:i + hop_length]\n",
        "        if len(segment) < hop_length:  # Ensure each segment is long enough\n",
        "            break\n",
        "        segments.append(segment)\n",
        "    return segments\n",
        "\n",
        "\n",
        "\n",
        "# Process files for both speakers and both talking\n",
        "def process_files(speaker1_file, speaker2_file, both_file):\n",
        "    # Load audio files\n",
        "    y1, sr1 = load_audio(speaker1_file)\n",
        "    y2, sr2 = load_audio(speaker2_file)\n",
        "    y_both, sr_both = load_audio(both_file)\n",
        "\n",
        "    # Extract pitch and plot FFT for each file\n",
        "    segments1 = []\n",
        "    segments2 = []\n",
        "    segments3 = []\n",
        "    #for y, sr, title in [(y1, sr1, 'Speaker 1'), (y2, sr2, 'Speaker 2')]:#, (y_both, sr_both, 'Both')]:\n",
        "    segments1 = split_to_Segments(y1, sr1, segment_length=0.5)\n",
        "    segments2 = split_to_Segments(y2, sr2, segment_length=0.5)\n",
        "    # plot_fft(y, sr, title)\n",
        "\n",
        "    pitch1 = []\n",
        "    pitch2 = []\n",
        "    pitch1 = extract_pitch(segments1 , sr1 , 'Speaker 1')\n",
        "    pitch2 = extract_pitch(segments2 , sr2 , 'Speaker 2')\n",
        "\n",
        "    # Extract MFCCs and create labeled dataset\n",
        "    mfcc1 = extract_mfcc(segments1 , sr1)   # Speaker 1 MFCCs\n",
        "    mfcc2 = extract_mfcc(segments2 , sr2)   # Speaker 2 MFCCs\n",
        "    #mfcc_both = extract_mfcc(y_both, sr_both)  # Both Speakers MFCCs\n",
        "\n",
        "    # Create labeled dataset (1 for speaker1, 2 for speaker2, 3 for both)\n",
        "    data = np.concatenate([mfcc1, mfcc2], axis=0)\n",
        "    labels = np.concatenate([np.ones(len(mfcc1)), 2 * np.ones(len(mfcc2))])\n",
        "\n",
        "    return data, labels\n",
        "\n",
        "# Train and evaluate model using Decision Trees\n",
        "def train_and_evaluate(data, labels):\n",
        "    # Split dataset into train (80%) and test (20%) sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train a Decision Tree Classifier\n",
        "    clf = DecisionTreeClassifier(random_state=42)\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(X_train.shape)\n",
        "    print(X_test.shape)\n",
        "\n",
        "# Main function to execute the pipeline\n",
        "def main():\n",
        "    speaker1_file = 'mom.mp3'\n",
        "    speaker2_file = 'son.mp3'\n",
        "    both_file = '0929.wav'\n",
        "\n",
        "    # Process the audio files and extract features\n",
        "    data, labels = process_files(speaker1_file, speaker2_file, both_file)\n",
        "    print(data.shape)\n",
        "    print(labels.shape )\n",
        "    # Train and evaluate the model\n",
        "    train_and_evaluate(data, labels)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "zkVcbZnjygSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install librosa pydub numpy\n"
      ],
      "metadata": {
        "id": "Nh7N_FHfaz2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# Load audio file\n",
        "def load_audio(file_path):\n",
        "    y, sr = librosa.load(file_path, sr=None)\n",
        "    return y, sr\n",
        "\n",
        "# Detect silent moments using librosa\n",
        "def detect_silence(y, sr, top_db=20, frame_length=2048, hop_length=512):\n",
        "    \"\"\"\n",
        "    Detects non-silent regions in the audio based on energy threshold (top_db).\n",
        "    Returns intervals where the audio is non-silent.\n",
        "    \"\"\"\n",
        "    non_silent_intervals = librosa.effects.split(y, top_db=top_db, frame_length=frame_length, hop_length=hop_length)\n",
        "    return non_silent_intervals\n",
        "\n",
        "# Remove silent moments from an audio file\n",
        "def remove_silence(file_path, output_path, top_db=20):\n",
        "    # Load the audio file\n",
        "    y, sr = load_audio(file_path)\n",
        "\n",
        "    print(len(y))\n",
        "\n",
        "    # Detect non-silent intervals\n",
        "    non_silent_intervals = detect_silence(y, sr, top_db=top_db)\n",
        "    print(len(non_silent_intervals))\n",
        "\n",
        "    # Load the audio using pydub for easier manipulation\n",
        "    audio = AudioSegment.from_file(file_path)\n",
        "    print(len(audio))\n",
        "\n",
        "    # Concatenate non-silent audio segments\n",
        "    non_silent_audio = AudioSegment.empty()\n",
        "    for start, end in non_silent_intervals:\n",
        "        start_ms = librosa.frames_to_time(start, sr=sr) * 1000  # Convert to milliseconds\n",
        "        end_ms = librosa.frames_to_time(end, sr=sr) * 1000      # Convert to milliseconds\n",
        "        non_silent_audio += audio[start_ms:end_ms]\n",
        "\n",
        "    # Export the resulting audio without silence\n",
        "    non_silent_audio.export(output_path, format=\"wav\")\n",
        "    print(f\"Audio without silence saved to {output_path}\")\n",
        "\n",
        "# Example usage\n",
        "input_file = \"0929.wav\"  # Path to input file\n",
        "output_file = \"output_no_silence.wav\"  # Path for output file\n",
        "\n",
        "remove_silence(input_file, output_file, top_db=20)\n"
      ],
      "metadata": {
        "id": "0xV-BTnjygV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydub ipywidgets\n"
      ],
      "metadata": {
        "id": "BTLyGo2YtCfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pydub import AudioSegment\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Audio\n",
        "import numpy as np\n",
        "import librosa\n",
        "\n",
        "# Load MP3 file\n",
        "audio = AudioSegment.from_mp3('/content/0929.mp3')\n",
        "\n",
        "# Define segment length (0.5 seconds = 500 milliseconds)\n",
        "segment_length_ms = 500\n",
        "segment_length_samples = int(0.5 * audio.frame_rate)\n",
        "\n",
        "# Split audio into 0.5-second segments\n",
        "total_duration_ms = len(audio)\n",
        "segments = [audio[i:i+segment_length_ms] for i in range(0, total_duration_ms, segment_length_ms)]\n",
        "\n",
        "# Initialize dataset lists\n",
        "audio_features = []  # To store feature vectors for each segment\n",
        "labels = []  # To store classifications for each segment\n",
        "\n",
        "# Define classification options\n",
        "options = ['mom', 'baby', 'both', 'silence (noise)']\n",
        "\n",
        "# Flag to control number of segments (set to True to process all, False to limit to 20 segments)\n",
        "process_all_segments = False  # Change this flag to True if you want to process all segments\n",
        "\n",
        "# Function to convert a segment to raw audio array using librosa\n",
        "def get_audio_features(segment, segment_index):\n",
        "    temp_file = f\"temp_{segment_index}.wav\"\n",
        "    segment.export(temp_file, format=\"wav\")\n",
        "\n",
        "    # Load audio segment as a numpy array using librosa\n",
        "    y, sr = librosa.load(temp_file, sr=None)\n",
        "    return y\n",
        "\n",
        "# Function to play a segment and ask for classification\n",
        "def classify_segment(segment, segment_index):\n",
        "    # Export and play the audio\n",
        "    temp_file = f\"temp_{segment_index}.wav\"\n",
        "    segment.export(temp_file, format=\"wav\")\n",
        "    display(Audio(temp_file, autoplay=True))\n",
        "\n",
        "    # Create a dropdown widget for classification\n",
        "    classification_dropdown = widgets.Dropdown(\n",
        "        options=['mom', 'baby', 'both', 'silence (noise)'],\n",
        "        value=None,\n",
        "        description='Classification:'\n",
        "    )\n",
        "    display(classification_dropdown)\n",
        "\n",
        "    return classification_dropdown\n",
        "\n",
        "# Function to collect classifications and features for the selected number of segments\n",
        "def classify_segments(segments, limit_segments):\n",
        "    classification_widgets = []\n",
        "\n",
        "    # Limit the segments if needed\n",
        "    max_segments = min(len(segments), limit_segments)\n",
        "\n",
        "    for i, segment in enumerate(segments[:max_segments]):\n",
        "        print(f\"Segment {i+1}/{max_segments}\")\n",
        "        classification_dropdown = classify_segment(segment, i)\n",
        "\n",
        "        # Get feature vector for the segment\n",
        "        features = get_audio_features(segment, i)\n",
        "        audio_features.append(features)  # Append features to dataset\n",
        "        classification_widgets.append(classification_dropdown)\n",
        "\n",
        "    # Button to save all classifications after user selects\n",
        "    save_button = widgets.Button(description=\"Save Classifications\")\n",
        "\n",
        "    def save_classifications(b):\n",
        "        for i, widget in enumerate(classification_widgets):\n",
        "            labels.append(widget.value)  # Append classification to dataset\n",
        "            print(f\"Segment {i+1} classified as: {widget.value}\")\n",
        "\n",
        "        # Optional: Save dataset to file\n",
        "        np.savez('audio_classification_dataset.npz', audio_features=audio_features, labels=labels)\n",
        "        print(\"Dataset saved as 'audio_classification_dataset.npz'.\")\n",
        "\n",
        "    save_button.on_click(save_classifications)\n",
        "    display(save_button)\n",
        "\n",
        "# Set the number of segments to process\n",
        "if process_all_segments:\n",
        "    classify_segments(segments, len(segments))  # Process all segments\n",
        "else:\n",
        "    classify_segments(segments, 50)  # Process only 20 segments\n"
      ],
      "metadata": {
        "id": "wayGh3733Vad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels"
      ],
      "metadata": {
        "id": "nAQkZG4D4xPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(audio_features[0])"
      ],
      "metadata": {
        "id": "tSxJU6zn4mVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "data = np.load('audio_classification_dataset.npz')\n",
        "audio_features = data['audio_features']\n",
        "labels = data['labels']\n",
        "\n",
        "# Define the mapping of string labels to integers\n",
        "label_mapping = {\n",
        "    'mom': 0,\n",
        "    'baby': 1,\n",
        "    'both': 2,\n",
        "    'silence (noise)': 3\n",
        "}\n",
        "\n",
        "# Convert the string labels to integer labels using the mapping\n",
        "integer_labels = np.array([label_mapping[label] for label in labels])\n",
        "\n",
        "# Check the converted labels\n",
        "print(integer_labels)\n"
      ],
      "metadata": {
        "id": "xkKLeuip5Jp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pydub import AudioSegment\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Audio\n",
        "import numpy as np\n",
        "import librosa\n",
        "\n",
        "# Load MP3 file\n",
        "audio = AudioSegment.from_mp3('/content/0929.MP3')\n",
        "\n",
        "# Define segment length (0.5 seconds = 500 milliseconds)\n",
        "segment_length_ms = 500\n",
        "segment_length_samples = int(0.5 * audio.frame_rate)\n",
        "\n",
        "# Split audio into 0.5-second segments\n",
        "total_duration_ms = len(audio)\n",
        "segments = [audio[i:i+segment_length_ms] for i in range(0, total_duration_ms, segment_length_ms)]\n",
        "\n",
        "# Initialize dataset lists\n",
        "mfcc_features = []  # To store MFCC feature vectors for each segment\n",
        "labels = []  # To store classifications for each segment\n",
        "\n",
        "# Define classification options\n",
        "options = ['mom', 'baby', 'both', 'silence (noise)']\n",
        "\n",
        "# Flag to control number of segments (set to True to process all, False to limit to 20 segments)\n",
        "process_all_segments = True  # Change this flag to True if you want to process all segments\n",
        "\n",
        "# Function to convert a segment to MFCC features using librosa\n",
        "def get_mfcc_features(segment, segment_index):\n",
        "    temp_file = f\"temp_{segment_index}.wav\"\n",
        "    segment.export(temp_file, format=\"wav\")\n",
        "\n",
        "    # Load audio segment as a numpy array using librosa\n",
        "    y, sr = librosa.load(temp_file, sr=None)\n",
        "\n",
        "    # Extract MFCC features (13 coefficients by default)\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "\n",
        "    # Take the mean across the time axis (axis=1) to create a feature vector\n",
        "    mfcc_mean = np.mean(mfcc, axis=1)\n",
        "\n",
        "    return mfcc\n",
        "\n",
        "# Function to play a segment and ask for classification\n",
        "def classify_segment(segment, segment_index):\n",
        "    # Export and play the audio\n",
        "    temp_file = f\"temp_{segment_index}.wav\"\n",
        "    segment.export(temp_file, format=\"wav\")\n",
        "    display(Audio(temp_file, autoplay=True))\n",
        "\n",
        "    # Create a dropdown widget for classification\n",
        "    classification_dropdown = widgets.Dropdown(\n",
        "        options=['mom', 'baby', 'both', 'silence (noise)'],\n",
        "        value=None,\n",
        "        description='Classification:'\n",
        "    )\n",
        "    display(classification_dropdown)\n",
        "\n",
        "    return classification_dropdown\n",
        "\n",
        "# Function to collect classifications and features for the selected number of segments\n",
        "def classify_segments(segments, limit_segments):\n",
        "    classification_widgets = []\n",
        "\n",
        "    # Limit the segments if needed\n",
        "    max_segments = min(len(segments), limit_segments)\n",
        "\n",
        "    for i, segment in enumerate(segments[:max_segments]):\n",
        "        print(f\"Segment {i+1}/{max_segments}\")\n",
        "        classification_dropdown = classify_segment(segment, i)\n",
        "\n",
        "        # Get MFCC feature vector for the segment\n",
        "        features = get_mfcc_features(segment, i)\n",
        "        mfcc_features.append(features)  # Append MFCC features to dataset\n",
        "        classification_widgets.append(classification_dropdown)\n",
        "\n",
        "    # Button to save all classifications after user selects\n",
        "    save_button = widgets.Button(description=\"Save Classifications\")\n",
        "\n",
        "    def save_classifications(b):\n",
        "        for i, widget in enumerate(classification_widgets):\n",
        "            labels.append(widget.value)  # Append classification to dataset\n",
        "            print(f\"Segment {i+1} classified as: {widget.value}\")\n",
        "\n",
        "        # Save the MFCC features and labels as a dataset\n",
        "        np.savez('audio_classification_dataset_mfcc.npz', mfcc_features=mfcc_features, labels=labels)\n",
        "        print(\"Dataset saved as 'audio_classification_dataset_mfcc.npz'.\")\n",
        "\n",
        "    save_button.on_click(save_classifications)\n",
        "    display(save_button)\n",
        "\n",
        "# Set the number of segments to process\n",
        "if process_all_segments:\n",
        "    classify_segments(segments, len(segments))  # Process all segments\n",
        "else:\n",
        "    classify_segments(segments, 50)  # Process only 20 segments\n"
      ],
      "metadata": {
        "id": "RtaC7Wto75je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(mfcc_features)):\n",
        "  #print(mfcc_features[i].shape)\n",
        "  if (mfcc_features[i].shape[0] != 13 or mfcc_features[i].shape[1] != 44 ):\n",
        "    print(mfcc_features[i].shape , i)"
      ],
      "metadata": {
        "id": "ueBSYyA23Xa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.savez('audio_classification_dataset_mfcc.npz', mfcc_features=mfcc_features[:488], labels=labels[:488])\n"
      ],
      "metadata": {
        "id": "GsC8FEek4OjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('labels.npy', labels)"
      ],
      "metadata": {
        "id": "KkuFL-pH5ccE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels"
      ],
      "metadata": {
        "id": "8oPTtD7e9REk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "data = np.load('audio_classification_dataset_mfcc.npz')\n",
        "mfcc_features = data['mfcc_features']\n",
        "labels = data['labels']\n",
        "\n",
        "# Define the mapping of string labels to integers\n",
        "label_mapping = {\n",
        "    'mom': 0,\n",
        "    'baby': 1,\n",
        "    'both': 2,\n",
        "    'silence (noise)': 3\n",
        "}\n",
        "\n",
        "# Convert the string labels to integer labels using the mapping\n",
        "integer_labels = np.array([label_mapping[label] for label in labels])\n",
        "flattened_mfccs = np.array([mfcc.flatten() for mfcc in mfcc_features])\n",
        "\n",
        "# Check the converted labels\n",
        "print(integer_labels)\n",
        "print(mfcc_features[0].shape)\n",
        "print(mfcc_features.shape)\n",
        "print(integer_labels.shape)\n",
        "print(flattened_mfccs.shape)"
      ],
      "metadata": {
        "id": "VXr3E9mL9aOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matrix_2d = np.mean(mfcc_features, axis=2)\n",
        "matrix_2d.shape , len(integer_labels)"
      ],
      "metadata": {
        "id": "WJzy8T6fU8Ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(mfcc_features[0][0]) , matrix_2d[0][0] , mfcc_features[0][0]"
      ],
      "metadata": {
        "id": "1rs_lyFMf9jN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "for target_class in range(0,4):\n",
        "  indexes = np.where(integer_labels == target_class)[0]\n",
        "\n",
        "  # Extract the corresponding elements from array a using the indexes\n",
        "  extracted_a = matrix_2d[indexes]\n",
        "  print(extracted_a.shape)\n",
        "\n",
        "  print('class :' , target_class , 'mean :' , np.mean(extracted_a))\n",
        "\n",
        "for i in range(extracted_a.shape[0]):\n",
        "  plt.plot(extracted_a[i])"
      ],
      "metadata": {
        "id": "hnBrj7idjJEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indexes = np.where(integer_labels == 3)[0]\n",
        "len(indexes)"
      ],
      "metadata": {
        "id": "Uj8JWoeXr3wF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indexes = np.where(integer_labels != 3)[0]\n",
        "\n",
        "# Filter both a and b using the indexes\n",
        "filtered_matrix_2d = matrix_2d[indexes]\n",
        "filtered_integer_labels = integer_labels[indexes]\n",
        "filtered_matrix_2d.shape"
      ],
      "metadata": {
        "id": "qEA_OJtEs0jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evaluate model using Decision Trees\n",
        "def train_and_evaluate(data, labels):\n",
        "    # Split dataset into train (80%) and test (20%) sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train a Decision Tree Classifier\n",
        "    #clf = DecisionTreeClassifier(random_state=42)\n",
        "    #clf.fit(X_train, y_train)\n",
        "    clf = RandomForestClassifier(n_estimators=200)\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(X_train.shape)\n",
        "    print(X_test.shape)\n",
        "    #print(y_train)\n",
        "    #print(y_test)\n"
      ],
      "metadata": {
        "id": "85N89fFa-kYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "#train_and_evaluate(data=matrix_2d,labels=integer_labels)\n",
        "train_and_evaluate(data=filtered_matrix_2d,labels=filtered_integer_labels)"
      ],
      "metadata": {
        "id": "UbES4wPtCtYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evaluate model using Decision Trees\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "def train_and_evaluateX(data, labels):\n",
        "    # Split dataset into train (80%) and test (20%) sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42,shuffle=True, stratify=None)\n",
        "\n",
        "    # Train a Decision Tree Classifier\n",
        "    #clf = DecisionTreeClassifier(random_state=42)\n",
        "    #clf.fit(X_train, y_train)\n",
        "    #clf = RandomForestClassifier(n_estimators=100)\n",
        "    #clf.fit(X_train, y_train)\n",
        "    # Create and train an XGBoost classifier\n",
        "    clf = XGBClassifier()\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(X_train.shape)\n",
        "    print(X_test.shape)\n",
        "    #print(y_train)\n",
        "    #print(y_test)\n"
      ],
      "metadata": {
        "id": "H4zaO5OhW5AY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_and_evaluateX(data=matrix_2d,labels=integer_labels)\n",
        "train_and_evaluateX(data=filtered_matrix_2d,labels=filtered_integer_labels)"
      ],
      "metadata": {
        "id": "hNHqb0n-W1If"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Train and evaluate model using Decision Trees\n",
        "def train_and_evaluate_net(data, labels):\n",
        "    n_mfcc = mfcc_features.shape[1]\n",
        "    time_steps = mfcc_features.shape[2]\n",
        "    print(n_mfcc)\n",
        "    print(time_steps)\n",
        "\n",
        "    # Split dataset into train (80%) and test (20%) sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(1, time_steps, n_mfcc )),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D((1, 1)),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dense(4, activation='softmax')  # 4 output classes (mom, baby, both, silence)\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(X_train.shape)\n",
        "    print(X_test.shape)\n",
        "    print(y_train)\n",
        "    print(y_test)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "I-ONPGrJYf7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(filtered_matrix_2d, filtered_integer_labels, test_size=0.2, random_state=42 ,shuffle=True, stratify=None)\n",
        "\n",
        "print(X_train.shape)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "#train_reshaped = X_train.reshape(-1, X_train.shape[1]*X_train.shape[2])\n",
        "#test_reshaped = X_test.reshape(-1, X_test.shape[1]*X_test.shape[2])\n",
        "#print(train_reshaped.shape)\n",
        "\n",
        "print('perform scaler')\n",
        "# Fit the scaler to the data and transform it\n",
        "train_data = scaler.fit_transform(X_train)\n",
        "test_data = scaler.fit_transform(X_test)\n",
        "\n",
        "#train_final = train_data.reshape(-1, X_train.shape[1],X_train.shape[2])\n",
        "#test_final = test_data.reshape(-1, X_test.shape[1],X_test.shape[2])\n",
        "#print(train_final.shape)\n",
        "\n",
        "n_mfcc = mfcc_features.shape[1]\n",
        "time_steps = mfcc_features.shape[2]\n",
        "\n",
        "# Build a CNN model\n",
        "#model = tf.keras.Sequential([\n",
        "#        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=( time_steps, n_mfcc , 1 )),\n",
        "#        tf.keras.layers.MaxPooling2D((1, 1)),\n",
        "#        tf.keras.layers.Dropout(0.1),\n",
        "#        tf.keras.layers.Conv2D(64, (1, 1), activation='relu'),\n",
        "#        tf.keras.layers.MaxPooling2D((1, 1)),\n",
        "#        tf.keras.layers.Flatten(),\n",
        "#        tf.keras.layers.Dense(128, activation='relu'),\n",
        "#        tf.keras.layers.Dropout(0.3),\n",
        "#        tf.keras.layers.Dense(4, activation='softmax')  # 4 output classes (mom, baby, both, silence)\n",
        "#    ])\n",
        "\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv1D(16, 2,1, activation='relu', input_shape=(  n_mfcc , 1 )),\n",
        "        #tf.keras.layers.Dropout(0.1),\n",
        "        tf.keras.layers.Conv1D(16, 3,1, activation='relu'),\n",
        "        #tf.keras.layers.Dropout(0.1),\n",
        "        tf.keras.layers.Conv1D(16, 4,1, activation='relu'),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(32, activation='relu'),\n",
        "        #tf.keras.layers.Dropout(0.1),\n",
        "        tf.keras.layers.Dense(4, activation='softmax')  # 4 output classes (mom, baby, both, silence)\n",
        "    ])\n",
        "\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_data, y_train, epochs=100, validation_data=(test_data, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(test_data, y_test, verbose=2)\n",
        "print(f'Test accuracy: {test_acc}')"
      ],
      "metadata": {
        "id": "4UetBJhfcxUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "\n",
        "# Assuming MFCCs are precomputed and normalized to shape (n_samples, n_mfcc, time_steps, 1)\n",
        "\n",
        "\n",
        "def build_model(input_shape, num_classes=4):\n",
        "    # Load pre-trained ResNet50 model (you can choose others like VGG)\n",
        "    base_model = ResNet50(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=input_shape\n",
        "    )\n",
        "\n",
        "    # Freeze the base model to retain pre-trained weights\n",
        "    base_model.trainable = False\n",
        "\n",
        "    # Build the classification model\n",
        "    model = models.Sequential()\n",
        "    model.add(base_model)\n",
        "\n",
        "    # Global pooling layer to reduce spatial dimensions\n",
        "    model.add(layers.GlobalAveragePooling2D())\n",
        "\n",
        "    # Add fully connected layers for classification\n",
        "    model.add(layers.Dense(128, activation='relu'))\n",
        "    model.add(layers.Dropout(0.5))  # Prevent overfitting\n",
        "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Define input shape: (n_mfcc, time_steps, 1)\n",
        "input_shape = ( n_mfcc, time_steps , 3)\n",
        "\n",
        "# Build and compile the model\n",
        "model = build_model(input_shape=input_shape, num_classes=4)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Sample training loop (assuming you have data ready)\n",
        "# Assuming X_train is your MFCC data and y_train are the labels\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n"
      ],
      "metadata": {
        "id": "lyuZkXE8VtSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import SqueezeNet\n",
        "\n",
        "\n",
        "def build_grayscale_model(input_shape, num_classes=4):\n",
        "    # Load pre-trained MobileNetV2 model, modifying the input to accept 1 channel (grayscale)\n",
        "    base_model = SqueezeNet(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=input_shape\n",
        "    )\n",
        "\n",
        "    # Freeze the base model to retain pre-trained weights\n",
        "    base_model.trainable = False\n",
        "\n",
        "    # Build the classification model\n",
        "    model = models.Sequential()\n",
        "    model.add(base_model)\n",
        "\n",
        "    # Global pooling layer to reduce spatial dimensions\n",
        "    model.add(layers.GlobalAveragePooling2D())\n",
        "\n",
        "    # Add fully connected layers for classification\n",
        "    model.add(layers.Dense(128, activation='relu'))\n",
        "    model.add(layers.Dropout(0.5))  # Prevent overfitting\n",
        "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Define input shape: (13, 44, 1) for grayscale input\n",
        "input_shape = (13, 44, 3)\n",
        "\n",
        "# Build and compile the model\n",
        "model = build_grayscale_model(input_shape=input_shape, num_classes=4)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Sample training loop (assuming you have data ready)\n",
        "# history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n"
      ],
      "metadata": {
        "id": "X1AcgSwhbujV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape , n_mfcc, time_steps,"
      ],
      "metadata": {
        "id": "7e-XwF748GLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_reshaped = X_train.reshape(-1, X_train.shape[1]*X_train.shape[2])\n",
        "data_reshaped.shape"
      ],
      "metadata": {
        "id": "G5CRmpyS-Drg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_reshaped = X_train.reshape(-1, X_train.shape[1]*X_train.shape[2])\n",
        "new = data_reshaped.reshape(-1, X_train.shape[1],X_train.shape[2])\n",
        "new.shape"
      ],
      "metadata": {
        "id": "j9gQ3HFJ_OoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mfcc_features.shape , integer_labels"
      ],
      "metadata": {
        "id": "R-DfnKbHgRXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jgKiX-D2jd9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n"
      ],
      "metadata": {
        "id": "YEJYDdKf-pNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_evaluate(data=flattened_mfccs,labels=integer_labels)"
      ],
      "metadata": {
        "id": "FXx_njbIEl10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_evaluateX(data=flattened_mfccs,labels=integer_labels)"
      ],
      "metadata": {
        "id": "g04ILmbbXUBk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}